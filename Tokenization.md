> The process of chopping up sentences into bite-sized pieces

### Steps
1. Break down text into smaller units called <u>tokens</u>
2. Each Token --mapped--> Numerical value
	- A numerical value that the model can process
3. Models learn to understand statistical relationship between tokens

Once a model is able to understand relationships between tokens, in the context of [[Large Language Models|LLMs]], the next object is to apply this understanding by predicting the next token given a series of token inputs

### Tokenization Techniques
1. Subword tokenization
	- [[Byte Pair Encoding|Byte Pair Encoding (BPE)]] #TODO 

There are specialized [[Tokenizers|tokenizers]] for processing math and code! #TODO  