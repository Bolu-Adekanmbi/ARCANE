> Set of all [[Tokenization#Token?|tokens]] that an [[Large Language Models|LLM]] can work with
