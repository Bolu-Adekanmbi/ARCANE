Highlight: Transformer models are able to *learn* **CONTEXT**

Transformers use a mathematical technique called [[self-attention]] to detect subtle ways in which elements in a sequence relate to each other
- This enables understanding of ...
	- ...how the end of a sentence connects to the beginning
	- ...how a starting word connects to the words that follow
	- ...how paragraphs connect with one another

Transformer architecture on some level <u>understand</u> **semantics**
- after seeing 